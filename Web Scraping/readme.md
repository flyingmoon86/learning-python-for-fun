# 🕸️ Python 爬虫 30 天学习计划

> 作者：杨启明  
> 目标：30天后能独立编写一个能爬取网页、分析数据、存储结果的完整爬虫项目。  
> 前提：有 C 语言基础、Python 基础。

---

## 🧭 阶段总览

| 阶段 | 时间 | 目标 |
|------|------|------|
| 第1阶段 | 第1～7天 | 掌握 Python 基础与网页结构 |
| 第2阶段 | 第8～14天 | 能爬取多页静态网站并保存数据 |
| 第3阶段 | 第15～21天 | 学会动态网页与反爬技巧 |
| 第4阶段 | 第22～26天 | 会进行数据清洗与可视化 |
| 第5阶段 | 第27～30天 | 完成综合爬虫实战项目 |

---

## 📘 第1阶段（第1～7天）：Python 与网络基础

> 🎯 目标：能写出基本脚本、理解网页结构与HTTP请求。

| 天数 | 学习内容 | 实践任务 |
|------|------------|------------|
| **Day 1** | Python 基础语法：变量、循环、函数、列表、字典 | 编写程序统计文本中每个单词出现次数 |
| **Day 2** | 文件操作（open/read/write），异常处理 | 从文件中读取10行内容并写入新文件 |
| **Day 3** | `requests`库入门：GET请求、响应内容 | 获取百度首页HTML并保存为`baidu.html` |
| **Day 4** | HTTP 请求结构：URL、Headers、Cookies | 打印响应头并理解`requests.get()`返回内容 |
| **Day 5** | HTML结构与`BeautifulSoup`解析 | 获取网页标题 `<title>` 内容 |
| **Day 6** | 使用`BeautifulSoup`提取文字、链接、图片 | 爬取豆瓣电影Top250名称和评分 |
| **Day 7** | 正则表达式（`re`库）提取数据 | 从HTML中提取所有URL并保存到文件 |

---

## 📗 第2阶段（第8～14天）：静态网页爬取实战

> 🎯 目标：能从多页网站提取结构化数据。

| 天数 | 学习内容 | 实践任务 |
|------|------------|------------|
| **Day 8** | 模拟Headers、防止403封禁 | 使用自定义User-Agent爬取知乎首页 |
| **Day 9** | 翻页爬取：分页参数与循环 | 爬取豆瓣Top250所有10页数据 |
| **Day 10** | 加入`time.sleep()`防封号 | 设置每页间隔1秒爬取 |
| **Day 11** | 保存数据到CSV (`csv`模块) | 将电影数据写入`movies.csv` |
| **Day 12** | 异常与容错处理 (`try/except`) | 增加失败重试机制 |
| **Day 13** | 封装爬虫函数 | 定义`get_html()`和`parse_html()`函数 |
| **Day 14** | 🧩 项目实战①：豆瓣电影Top250爬虫 | 完整爬取并导出CSV文件 |

---

## 📙 第3阶段（第15～21天）：动态网页与反爬机制

> 🎯 目标：掌握JS渲染网页爬取、模拟登录、Cookies操作。

| 天数 | 学习内容 | 实践任务 |
|------|------------|------------|
| **Day 15** | 动态网页原理与AJAX | 分析知乎热榜接口API |
| **Day 16** | `selenium`入门：自动化浏览器 | 打开B站首页并截屏 |
| **Day 17** | 登录与Cookies维持 | 模拟登录某论坛并打印用户名 |
| **Day 18** | 验证码与延迟加载机制 | 分析常见验证码类型 |
| **Day 19** | 反爬策略：随机User-Agent、代理IP | 实现随机Header与代理函数 |
| **Day 20** | 并发爬取 (`threading` / `aiohttp`基础) | 实现多线程图片下载爬虫 |
| **Day 21** | 🧩 项目实战②：B站热搜视频爬虫 | 提取热搜标题、播放量并存为CSV |

---

## 📒 第4阶段（第22～26天）：数据清洗与可视化

> 🎯 目标：能用`pandas`和`matplotlib`处理与展示数据。

| 天数 | 学习内容 | 实践任务 |
|------|------------|------------|
| **Day 22** | `pandas`读取CSV、基本统计 | 计算电影评分平均值 |
| **Day 23** | 数据清洗与筛选 | 筛选评分>9.0的电影 |
| **Day 24** | `matplotlib`绘图 | 绘制评分分布柱状图 |
| **Day 25** | 综合分析项目 | 爬取某市房价并绘制走势图 |
| **Day 26** | 数据库存储 (`sqlite3`) | 将数据写入SQLite并查询 |

---

## 📔 第5阶段（第27～30天）：综合实战与提升

> 🎯 目标：整合爬虫 + 清洗 + 分析 + 存储。

| 天数 | 学习内容 | 实践任务 |
|------|------------|------------|
| **Day 27** | 设计完整项目流程（采集→清洗→分析→存储） | 规划一个选题并写项目框架 |
| **Day 28** | 🧩 项目实战③：知乎热榜趋势分析 | 爬取热榜数据并绘制热度变化图 |
| **Day 29** | 优化与扩展：代理、异常处理、定时任务 | 加入代理与自动运行功能 |
| **Day 30** | 总结复盘 | 整理代码与笔记，上传GitHub或打包保存 |

---

## 🧰 常用库与工具

| 分类 | 库 / 工具 | 用途 |
|------|-------------|------|
| 网络请求 | `requests`, `aiohttp` | 获取网页内容 |
| 网页解析 | `BeautifulSoup`, `lxml`, `re` | 提取网页信息 |
| 自动化 | `selenium`, `requests_html` | 爬取动态网页 |
| 数据处理 | `pandas`, `numpy` | 数据清洗与分析 |
| 可视化 | `matplotlib`, `seaborn` | 数据图表展示 |
| 存储 | `csv`, `sqlite3`, `json` | 保存与读取数据 |
| 调试 | Chrome 开发者工具、Postman | 分析网页结构与请求 |

---

## 💡 学习建议

- 🧠 每天学习 1~1.5 小时，坚持输出代码。
- ⚙️ 每个库都动手写示例，而不仅是看。
- 🔍 善用浏览器「检查元素」「Network」面板。
- 🐢 慢比快更重要——理解比记忆更关键。
- 📓 每个阶段后写学习笔记与反思。
- ⚖️ 遵守爬虫礼仪：尊重网站 `robots.txt`。

---

## 🎯 最终目标

完成一个完整的爬虫项目，包含：
1. 自动获取网页数据；
2. 清洗与存储；
3. 统计分析与可视化；
4. 程序可复用、可维护。

> 💬 坚持30天，你将能独立编写中小型数据采集项目，轻松应对学术研究、数据分析或自动化任务。
